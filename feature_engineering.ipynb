{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b97c56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data Loaded Successfully!\n",
      "Subnational 1 Shape: (288, 30)\n",
      "Subnational 2 Shape: (5328, 31)\n",
      "\n",
      "--- Subnational 1 Columns ---\n",
      "['country', 'subnational1', 'threshold', 'area_ha', 'extent_2000_ha', 'extent_2010_ha', 'gain_2000-2020_ha', 'tc_loss_ha_2001', 'tc_loss_ha_2002', 'tc_loss_ha_2003', 'tc_loss_ha_2004', 'tc_loss_ha_2005', 'tc_loss_ha_2006', 'tc_loss_ha_2007', 'tc_loss_ha_2008', 'tc_loss_ha_2009', 'tc_loss_ha_2010', 'tc_loss_ha_2011', 'tc_loss_ha_2012', 'tc_loss_ha_2013', 'tc_loss_ha_2014', 'tc_loss_ha_2015', 'tc_loss_ha_2016', 'tc_loss_ha_2017', 'tc_loss_ha_2018', 'tc_loss_ha_2019', 'tc_loss_ha_2020', 'tc_loss_ha_2021', 'tc_loss_ha_2022', 'tc_loss_ha_2023']\n",
      "\n",
      "--- Subnational 2 Columns ---\n",
      "['country', 'subnational1', 'subnational2', 'threshold', 'area_ha', 'extent_2000_ha', 'extent_2010_ha', 'gain_2000-2020_ha', 'tc_loss_ha_2001', 'tc_loss_ha_2002', 'tc_loss_ha_2003', 'tc_loss_ha_2004', 'tc_loss_ha_2005', 'tc_loss_ha_2006', 'tc_loss_ha_2007', 'tc_loss_ha_2008', 'tc_loss_ha_2009', 'tc_loss_ha_2010', 'tc_loss_ha_2011', 'tc_loss_ha_2012', 'tc_loss_ha_2013', 'tc_loss_ha_2014', 'tc_loss_ha_2015', 'tc_loss_ha_2016', 'tc_loss_ha_2017', 'tc_loss_ha_2018', 'tc_loss_ha_2019', 'tc_loss_ha_2020', 'tc_loss_ha_2021', 'tc_loss_ha_2022', 'tc_loss_ha_2023']\n",
      "\n",
      "Combined dataset shape: (5616, 31)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Load and inspect the two datasets\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "# Load both CSVs\n",
    "s1 = pd.read_csv(\"subnational_1_tree_cover_loss.csv\")\n",
    "s2 = pd.read_csv(\"subnational_2_tree_cover_loss.csv\")\n",
    "\n",
    "print(\" Data Loaded Successfully!\")\n",
    "print(f\"Subnational 1 Shape: {s1.shape}\")\n",
    "print(f\"Subnational 2 Shape: {s2.shape}\")\n",
    "\n",
    "# Quick look at columns\n",
    "print(\"\\n--- Subnational 1 Columns ---\")\n",
    "print(s1.columns.tolist())\n",
    "print(\"\\n--- Subnational 2 Columns ---\")\n",
    "print(s2.columns.tolist())\n",
    "\n",
    "# Combine both sheets into one DataFrame\n",
    "combined = pd.concat([s1, s2], ignore_index=True)\n",
    "print(f\"\\nCombined dataset shape: {combined.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e47e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Basic cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Clean and standardize column names\n",
    "\n",
    "combined.columns = (\n",
    "    combined.columns.str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(\" \", \"_\")\n",
    "    .str.replace(\"__+\", \"_\", regex=True)\n",
    ")\n",
    "\n",
    "# Drop duplicates and null-heavy columns\n",
    "combined = combined.drop_duplicates()\n",
    "combined = combined.dropna(axis=1, thresh=len(combined) * 0.5)\n",
    "\n",
    "print(\" Basic cleaning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec913498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Missing numeric values imputed using KNN.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Handle missing values\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np\n",
    "\n",
    "numeric_cols = combined.select_dtypes(include=np.number).columns\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "combined[numeric_cols] = imputer.fit_transform(combined[numeric_cols])\n",
    "print(\" Missing numeric values imputed using KNN.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff2bf2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Label encoding completed for categorical columns.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. Label Encode categorical variables\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_cols = combined.select_dtypes(exclude=np.number).columns\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    combined[col] = le.fit_transform(combined[col].astype(str))\n",
    "\n",
    "print(\" Label encoding completed for categorical columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64baa099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['country', 'subnational1', 'threshold', 'area_ha', 'extent_2000_ha', 'extent_2010_ha', 'gain_2000-2020_ha', 'tc_loss_ha_2001', 'tc_loss_ha_2002', 'tc_loss_ha_2003', 'tc_loss_ha_2004', 'tc_loss_ha_2005', 'tc_loss_ha_2006', 'tc_loss_ha_2007', 'tc_loss_ha_2008', 'tc_loss_ha_2009', 'tc_loss_ha_2010', 'tc_loss_ha_2011', 'tc_loss_ha_2012', 'tc_loss_ha_2013', 'tc_loss_ha_2014', 'tc_loss_ha_2015', 'tc_loss_ha_2016', 'tc_loss_ha_2017', 'tc_loss_ha_2018', 'tc_loss_ha_2019', 'tc_loss_ha_2020', 'tc_loss_ha_2021', 'tc_loss_ha_2022', 'tc_loss_ha_2023', 'subnational2']\n"
     ]
    }
   ],
   "source": [
    "print(combined.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8848f5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Derived ratios and density metrics calculated.\n"
     ]
    }
   ],
   "source": [
    "# --- Domain-specific feature engineering ---\n",
    "\n",
    "# Ensure key columns exist (adjusted for your dataset)\n",
    "required = [\"extent_2000_ha\", \"gain_2000-2020_ha\"]\n",
    "for col in required:\n",
    "    if col not in combined.columns:\n",
    "        print(f\" Missing column: {col}\")\n",
    "\n",
    "# Calculate total tree cover loss from yearly columns\n",
    "loss_cols = [c for c in combined.columns if \"tc_loss_ha_\" in c]\n",
    "combined[\"tree_cover_loss_total\"] = combined[loss_cols].sum(axis=1)\n",
    "\n",
    "# Compute ratios and density metrics\n",
    "combined[\"loss_gain_ratio\"] = (combined[\"tree_cover_loss_total\"] + 1) / (combined[\"gain_2000-2020_ha\"] + 1)\n",
    "combined[\"loss_extent_ratio\"] = (combined[\"tree_cover_loss_total\"] + 1) / (combined[\"extent_2000_ha\"] + 1)\n",
    "combined[\"gain_extent_ratio\"] = (combined[\"gain_2000-2020_ha\"] + 1) / (combined[\"extent_2000_ha\"] + 1)\n",
    "\n",
    "print(\" Derived ratios and density metrics calculated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "721b0590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GDI calculated and categorized successfully.\n"
     ]
    }
   ],
   "source": [
    "# 6. Compute Green Deficit Index (GDI)\n",
    "required = [\"extent_2000_ha\", \"gain_2000-2020_ha\", \"tree_cover_loss_total\"]\n",
    "\n",
    "if all(col in combined.columns for col in required):\n",
    "    combined['GDI'] = (\n",
    "        (combined['tree_cover_loss_total'] - combined['gain_2000-2020_ha'])\n",
    "        / (combined['extent_2000_ha'] + 1)\n",
    "    )\n",
    "\n",
    "    # Categorize based on defined thresholds\n",
    "    def categorize_gdi(value):\n",
    "        if value <= -5:\n",
    "            return \"Excellent (Net Gain)\"\n",
    "        elif -5 < value <= 0:\n",
    "            return \"Acceptable\"\n",
    "        elif 0 < value <= 10:\n",
    "            return \"Concerning\"\n",
    "        else:\n",
    "            return \"High-Risk\"\n",
    "\n",
    "    combined['GDI_Category'] = combined['GDI'].apply(categorize_gdi)\n",
    "    print(\" GDI calculated and categorized successfully.\")\n",
    "else:\n",
    "    print(\" Required columns missing for GDI computation.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15873bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top 10 features correlated with GDI:\n",
      "\n",
      "GDI                      1.000000\n",
      "loss_gain_ratio          0.043219\n",
      "tc_loss_ha_2007          0.037334\n",
      "tc_loss_ha_2006          0.037154\n",
      "tc_loss_ha_2005          0.036179\n",
      "tc_loss_ha_2008          0.035928\n",
      "tc_loss_ha_2023          0.035795\n",
      "tc_loss_ha_2011          0.035616\n",
      "tc_loss_ha_2003          0.034932\n",
      "tree_cover_loss_total    0.034897\n",
      "Name: GDI, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 7. Generate correlation-based insights\n",
    "\n",
    "# Compute numeric correlations only\n",
    "corr_matrix = combined.select_dtypes(include=['float64', 'int64']).corr()\n",
    "\n",
    "if \"GDI\" in corr_matrix.columns:\n",
    "    strong_corr = corr_matrix[\"GDI\"].sort_values(ascending=False).head(10)\n",
    "    print(\"\\n Top 10 features correlated with GDI:\\n\")\n",
    "    print(strong_corr)\n",
    "else:\n",
    "    print(\" 'GDI' not found in correlation matrix.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "140ef630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GREEN DEFICIT INDEX SUMMARY\n",
      "---------------------------------------------\n",
      " Average GDI: -40.57\n",
      " Minimum GDI: -6904.00\n",
      " Maximum GDI: 0.33\n",
      "\n",
      " Category Distribution:\n",
      "Acceptable              3485\n",
      "Excellent (Net Gain)    1104\n",
      "Concerning              1027\n",
      "Name: GDI_Category, dtype: int64\n",
      "\n",
      " Interpretation:\n",
      " - Negative GDI → Net Green Gain (Good ecological balance).\n",
      " - Positive GDI → Net Green Deficit (Loss per hectare, needs reforestation).\n",
      " - Higher GDI → Higher ecological risk requiring urgent mitigation.\n"
     ]
    }
   ],
   "source": [
    "# 9. Summary Insights\n",
    "# ===========================================\n",
    "if \"GDI\" in combined.columns:\n",
    "    print(\"\\n GREEN DEFICIT INDEX SUMMARY\")\n",
    "    print(\"-\" * 45)\n",
    "    print(f\" Average GDI: {combined['GDI'].mean():.2f}\")\n",
    "    print(f\" Minimum GDI: {combined['GDI'].min():.2f}\")\n",
    "    print(f\" Maximum GDI: {combined['GDI'].max():.2f}\")\n",
    "\n",
    "    print(\"\\n Category Distribution:\")\n",
    "    print(combined['GDI_Category'].value_counts())\n",
    "\n",
    "    print(\"\\n Interpretation:\")\n",
    "    print(\" - Negative GDI → Net Green Gain (Good ecological balance).\")\n",
    "    print(\" - Positive GDI → Net Green Deficit (Loss per hectare, needs reforestation).\")\n",
    "    print(\" - Higher GDI → Higher ecological risk requiring urgent mitigation.\")\n",
    "else:\n",
    "    print(\" Could not compute summary — GDI missing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f96e6f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved as feature_engineered_greenpulse.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 8. Export clean + feature engineered dataset\n",
    "\n",
    "combined.to_csv(\"feature_engineered_greenpulse.csv\", index=False)\n",
    "print(\" Saved as feature_engineered_greenpulse.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
